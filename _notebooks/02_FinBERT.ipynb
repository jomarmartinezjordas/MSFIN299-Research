{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using FinBERT\n",
    "This paper uses the FinBERT model that has been pre-trained by ProsusAI specifically for financial sentiment analysis. It is based on the BERT architecture and has been pre-trained on a large corpus of financial news articles and reports, making it highly specialized for financial language.  FinBERT is an open-source project making it readily available to the public. \n",
    "\n",
    "Among the limitations for this study is the computing power hence, this study shall stick with the un-tuned FinBERT model. Given the small relatively small corpus size, training it However, for other tasks, such as training a large-scale language model or conducting research on rare linguistic phenomena, a larger corpus may be necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing source directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing the CSV files\n",
    "dir_path = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_exports'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the input into the FinBERT Model\n",
    "\n",
    "_Important Note: The codes below are optimized for Macbooks (M1,M2) to make use of the GPU for the task._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Document level_\n",
    "\n",
    "In this section, we will use the FinBERT model to perform sentiment analysis on financial text documents at a document level. \n",
    "\n",
    "Each text file containing the scrubbed MD&A of each company for each year shall be subjected into the FinBERT model to generate a sentiment score of either 1, 0, or -1 interpreted as Positive, Neutral, or Negative respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FinBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Define the sentiment classes and scores\n",
    "sentiment_classes = ['positive', 'neutral', 'negative']\n",
    "sentiment_scores = [1, 0, -1]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results_doc = []\n",
    "\n",
    "# Iterate through all the text files in the directory\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        \n",
    "        # Read the contents of the text file\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read().replace('\\n', '')\n",
    "        \n",
    "        # Tokenize the text using the BERT tokenizer\n",
    "        tokens = tokenizer.encode_plus(text, max_length=512, truncation=True, padding='max_length',\n",
    "                                       add_special_tokens=True, return_tensors='pt')\n",
    "        device = torch.device('mps')\n",
    "        model.to(device)\n",
    "        tokens.to(device)        \n",
    "        \n",
    "        # Get the model's output for the tokenized text\n",
    "        output = model(**tokens)\n",
    "        \n",
    "        # Apply softmax to the logits output tensor of our model (in index 0) across dimension -1\n",
    "        probs = F.softmax(output[0], dim=-1)\n",
    "        \n",
    "        # Get the index of the predicted sentiment class\n",
    "        pred_class_idx = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        # Map the predicted sentiment class to a sentiment score and interpretation\n",
    "        sentiment_score = sentiment_scores[pred_class_idx.item()]\n",
    "        interpretation = sentiment_classes[pred_class_idx.item()]\n",
    "\n",
    "        # Extract ticker and year from the filename using regex\n",
    "        ticker = re.findall(r'^([A-Za-z]+)_\\d{4}\\.txt$', filename)[0]\n",
    "        year = re.findall(r'^[A-Za-z]+_(\\d{4})\\.txt$', filename)[0]\n",
    "        \n",
    "        # Add the sentiment score and interpretation to the list of results\n",
    "        results_doc.append({'file_name': filename, 'ticker': ticker, 'year': year, 'sentiment_score': sentiment_score, 'interpretation': interpretation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>interpretation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AC_2021.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ACEN_2021.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2021</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGI_2021.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MEG_2021.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AEV_2021.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MEG_2020.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AEV_2020.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AC_2020.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ACEN_2020.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGI_2020.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2020</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AC_2019.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MEG_2019.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AEV_2019.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGI_2019.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ACEN_2019.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2019</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MEG_2018.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ACEN_2018.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AEV_2018.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGI_2018.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AC_2018.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AC_2017.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AGI_2017.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AEV_2017.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEG_2017.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name ticker  year  sentiment_score interpretation\n",
       "8     AC_2021.txt     AC  2021                1       positive\n",
       "15  ACEN_2021.txt   ACEN  2021               -1       negative\n",
       "3    AGI_2021.txt    AGI  2021                1       positive\n",
       "18   MEG_2021.txt    MEG  2021                1       positive\n",
       "10   AEV_2021.txt    AEV  2021                1       positive\n",
       "17   MEG_2020.txt    MEG  2020                0        neutral\n",
       "11   AEV_2020.txt    AEV  2020                0        neutral\n",
       "9     AC_2020.txt     AC  2020                0        neutral\n",
       "16  ACEN_2020.txt   ACEN  2020                1       positive\n",
       "4    AGI_2020.txt    AGI  2020               -1       negative\n",
       "7     AC_2019.txt     AC  2019                0        neutral\n",
       "19   MEG_2019.txt    MEG  2019                1       positive\n",
       "14   AEV_2019.txt    AEV  2019                0        neutral\n",
       "2    AGI_2019.txt    AGI  2019                1       positive\n",
       "23  ACEN_2019.txt   ACEN  2019               -1       negative\n",
       "20   MEG_2018.txt    MEG  2018                1       positive\n",
       "22  ACEN_2018.txt   ACEN  2018                0        neutral\n",
       "12   AEV_2018.txt    AEV  2018                1       positive\n",
       "1    AGI_2018.txt    AGI  2018                1       positive\n",
       "6     AC_2018.txt     AC  2018                1       positive\n",
       "13    AC_2017.txt     AC  2017                1       positive\n",
       "21   AGI_2017.txt    AGI  2017                1       positive\n",
       "5    AEV_2017.txt    AEV  2017                0        neutral\n",
       "0    MEG_2017.txt    MEG  2017                1       positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with the sentiment score and interpretation for each file\n",
    "df_results_doc = pd.DataFrame(results_doc)\n",
    "\n",
    "# Print the results dataframe\n",
    "df = df_results_doc.sort_values('year', ascending=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sentence  level_\n",
    "Meanwhile, in this section, we will use the FinBERT model to perform sentiment analysis on financial text documents at a sentence level. The FinBERT model shall be used and iterated for each sentence in the text file to generate a sentiment score of either 1, 0, or -1. \n",
    "\n",
    "On a per file basis, the total number of positive, neutral, and negative sentences will be subjected to the following formula to get a sentiment score for the file. \n",
    "  \n",
    "$$Sentiment\\ Score_{corpus} = \\frac{Positive\\ sentence_{count} - Negative\\ sentence_{count}}{Total\\ Number\\ of\\ Sentences}$$\n",
    "  \n",
    "  \n",
    "It shall then be interpreted as follows\n",
    "  \n",
    "  \n",
    "$$\n",
    "\\text{Sentiment} =\n",
    "\\begin{cases}\n",
    "\\text{if } 1 > \\text{Sentiment score} > 0.05,\\text{Positive} \\\\\n",
    "\\text{if } 0.05 > \\text{Sentiment score} > -0.05,\\text{Neutral} \\\\\n",
    "\\text{if } -0.05 > \\text{Sentiment score} > -1,\\text{Negative} \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy English language model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Load the FinBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Define the sentiment scores and classes\n",
    "sentiment_scores_sent = [1, 0, -1]\n",
    "sentiment_classes_sent = ['positive', 'neutral', 'negative']\n",
    "\n",
    "# Initialize the results list and counters\n",
    "results_sent = []\n",
    "pos_count = 0\n",
    "neu_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "# Loop over each text file in the directory\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "        # Read the contents of the text file\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Use Spacy to split the text into sentences\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # Initialize the sentence-level results list and counters\n",
    "        sentence_results = []\n",
    "        pos_sent_count = 0\n",
    "        neu_sent_count = 0\n",
    "        neg_sent_count = 0\n",
    "\n",
    "        # Loop over each sentence in the text file\n",
    "        for sentence in sentences:\n",
    "            # Tokenize the sentence using the FinBERT tokenizer\n",
    "            tokens = tokenizer.encode_plus(sentence, max_length=512, truncation=True, padding='max_length',\n",
    "                                           add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "            device = torch.device('mps')\n",
    "            model.to(device)\n",
    "            tokens.to(device)\n",
    "\n",
    "            # Get the model's output for the tokenized sentence\n",
    "            output = model(**tokens.to(device))\n",
    "\n",
    "            # Apply softmax to the logits output tensor of our model (in index 0) across dimension -1\n",
    "            probs = F.softmax(output[0], dim=-1)\n",
    "\n",
    "            # Get the index of the predicted sentiment class\n",
    "            pred_class_idx = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Map the predicted sentiment class to a sentiment score and interpretation\n",
    "            sentiment_score = sentiment_scores_sent[pred_class_idx.item()]\n",
    "            if sentiment_score < -0.05:\n",
    "                interpretation = 'negative'\n",
    "            elif sentiment_score > 0.05:\n",
    "                interpretation = 'positive'\n",
    "            else:\n",
    "                interpretation = 'neutral'\n",
    "\n",
    "            # Add the sentiment score and interpretation to the sentence-level results list\n",
    "            sentence_results.append({'sentence': sentence.strip(), 'sentiment_score': sentiment_score, 'interpretation': interpretation})\n",
    "\n",
    "            # Update the sentence-level sentiment counters\n",
    "            if interpretation == 'positive':\n",
    "                pos_sent_count += 1\n",
    "            elif interpretation == 'neutral':\n",
    "                neu_sent_count += 1\n",
    "            elif interpretation == 'negative':\n",
    "                neg_sent_count += 1\n",
    "\n",
    "        # Calculate the overall sentiment score for the corpus\n",
    "        sentiment_score_corpus = (pos_sent_count - neg_sent_count) / len(sentences)\n",
    "\n",
    "        # Extract ticker and year from the filename using regex\n",
    "        ticker = re.findall(r'^([A-Za-z]+)_\\d{4}\\.txt$', filename)[0]\n",
    "        year = re.findall(r'^[A-Za-z]+_(\\d{4})\\.txt$', filename)[0]\n",
    "\n",
    "        # Add the sentence-level and file-level results to the results list\n",
    "        results_sent.append({'file_name': filename, 'ticker': ticker, 'year': year, \n",
    "                              'sentences': sentence_results, \n",
    "                              'positive_sent_count': pos_sent_count,\n",
    "                              'neutral_sent_count': neu_sent_count,\n",
    "                              'negative_sent_count': neg_sent_count,\n",
    "                              'sentiment_score_corpus': sentiment_score_corpus,\n",
    "                              'corpus_interpretation': 'positive' if sentiment_score_corpus > 0.05 else ('neutral' if -0.05 <= sentiment_score_corpus <= 0.05 else 'negative')})\n",
    "        \n",
    "        # Update the file-level sentiment counters\n",
    "        pos_count += pos_sent_count\n",
    "        neu_count += neu_sent_count\n",
    "        neg_count += neg_sent_count\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>sentences</th>\n",
       "      <th>positive_sent_count</th>\n",
       "      <th>neutral_sent_count</th>\n",
       "      <th>negative_sent_count</th>\n",
       "      <th>sentiment_score_corpus</th>\n",
       "      <th>corpus_interpretation</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AC_2021.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2021</td>\n",
       "      <td>[{'sentence': 'Ayala Corporation’s net profits...</td>\n",
       "      <td>110</td>\n",
       "      <td>46</td>\n",
       "      <td>68</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>positive</td>\n",
       "      <td>224</td>\n",
       "      <td>5748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ACEN_2021.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2021</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>54</td>\n",
       "      <td>26</td>\n",
       "      <td>73</td>\n",
       "      <td>-0.124183</td>\n",
       "      <td>negative</td>\n",
       "      <td>153</td>\n",
       "      <td>4303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGI_2021.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2021</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>positive</td>\n",
       "      <td>100</td>\n",
       "      <td>2797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MEG_2021.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2021</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>neutral</td>\n",
       "      <td>51</td>\n",
       "      <td>1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AEV_2021.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2021</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>201</td>\n",
       "      <td>24</td>\n",
       "      <td>88</td>\n",
       "      <td>0.361022</td>\n",
       "      <td>positive</td>\n",
       "      <td>313</td>\n",
       "      <td>8341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MEG_2020.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2020</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>negative</td>\n",
       "      <td>52</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AEV_2020.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2020</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>155</td>\n",
       "      <td>61</td>\n",
       "      <td>52</td>\n",
       "      <td>0.384328</td>\n",
       "      <td>positive</td>\n",
       "      <td>268</td>\n",
       "      <td>7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AC_2020.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2020</td>\n",
       "      <td>[{'sentence': 'The further easing of quarantin...</td>\n",
       "      <td>113</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>positive</td>\n",
       "      <td>266</td>\n",
       "      <td>6637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ACEN_2020.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2020</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>positive</td>\n",
       "      <td>82</td>\n",
       "      <td>2176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGI_2020.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2020</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>62</td>\n",
       "      <td>51</td>\n",
       "      <td>45</td>\n",
       "      <td>0.107595</td>\n",
       "      <td>positive</td>\n",
       "      <td>158</td>\n",
       "      <td>4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AC_2019.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2019</td>\n",
       "      <td>[{'sentence': 'Ayala Corporation’s full year e...</td>\n",
       "      <td>113</td>\n",
       "      <td>56</td>\n",
       "      <td>69</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>positive</td>\n",
       "      <td>238</td>\n",
       "      <td>5910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MEG_2019.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2019</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>positive</td>\n",
       "      <td>55</td>\n",
       "      <td>1687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AEV_2019.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2019</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>156</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "      <td>0.401606</td>\n",
       "      <td>positive</td>\n",
       "      <td>249</td>\n",
       "      <td>6622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGI_2019.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2019</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>79</td>\n",
       "      <td>14</td>\n",
       "      <td>41</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>positive</td>\n",
       "      <td>134</td>\n",
       "      <td>3555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ACEN_2019.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2019</td>\n",
       "      <td>[{'sentence': 'The Company posted consolidated...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>negative</td>\n",
       "      <td>66</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MEG_2018.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2018</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>positive</td>\n",
       "      <td>53</td>\n",
       "      <td>1868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ACEN_2018.txt</td>\n",
       "      <td>ACEN</td>\n",
       "      <td>2018</td>\n",
       "      <td>[{'sentence': 'The Company posted consolidated...</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.159420</td>\n",
       "      <td>negative</td>\n",
       "      <td>69</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AEV_2018.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2018</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>147</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>positive</td>\n",
       "      <td>207</td>\n",
       "      <td>5774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGI_2018.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2018</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>75</td>\n",
       "      <td>14</td>\n",
       "      <td>49</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>positive</td>\n",
       "      <td>138</td>\n",
       "      <td>3351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AC_2018.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2018</td>\n",
       "      <td>[{'sentence': 'Ayala Corporation’s net income ...</td>\n",
       "      <td>124</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>0.225225</td>\n",
       "      <td>positive</td>\n",
       "      <td>222</td>\n",
       "      <td>6091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AC_2017.txt</td>\n",
       "      <td>AC</td>\n",
       "      <td>2017</td>\n",
       "      <td>[{'sentence': 'Ayala Corporation recorded a ne...</td>\n",
       "      <td>119</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>positive</td>\n",
       "      <td>202</td>\n",
       "      <td>5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AGI_2017.txt</td>\n",
       "      <td>AGI</td>\n",
       "      <td>2017</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>93</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>positive</td>\n",
       "      <td>150</td>\n",
       "      <td>3605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AEV_2017.txt</td>\n",
       "      <td>AEV</td>\n",
       "      <td>2017</td>\n",
       "      <td>[{'sentence': 'The following discussion and an...</td>\n",
       "      <td>135</td>\n",
       "      <td>28</td>\n",
       "      <td>138</td>\n",
       "      <td>-0.009967</td>\n",
       "      <td>neutral</td>\n",
       "      <td>301</td>\n",
       "      <td>7513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEG_2017.txt</td>\n",
       "      <td>MEG</td>\n",
       "      <td>2017</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>positive</td>\n",
       "      <td>53</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name ticker  year  \\\n",
       "8     AC_2021.txt     AC  2021   \n",
       "15  ACEN_2021.txt   ACEN  2021   \n",
       "3    AGI_2021.txt    AGI  2021   \n",
       "18   MEG_2021.txt    MEG  2021   \n",
       "10   AEV_2021.txt    AEV  2021   \n",
       "17   MEG_2020.txt    MEG  2020   \n",
       "11   AEV_2020.txt    AEV  2020   \n",
       "9     AC_2020.txt     AC  2020   \n",
       "16  ACEN_2020.txt   ACEN  2020   \n",
       "4    AGI_2020.txt    AGI  2020   \n",
       "7     AC_2019.txt     AC  2019   \n",
       "19   MEG_2019.txt    MEG  2019   \n",
       "14   AEV_2019.txt    AEV  2019   \n",
       "2    AGI_2019.txt    AGI  2019   \n",
       "23  ACEN_2019.txt   ACEN  2019   \n",
       "20   MEG_2018.txt    MEG  2018   \n",
       "22  ACEN_2018.txt   ACEN  2018   \n",
       "12   AEV_2018.txt    AEV  2018   \n",
       "1    AGI_2018.txt    AGI  2018   \n",
       "6     AC_2018.txt     AC  2018   \n",
       "13    AC_2017.txt     AC  2017   \n",
       "21   AGI_2017.txt    AGI  2017   \n",
       "5    AEV_2017.txt    AEV  2017   \n",
       "0    MEG_2017.txt    MEG  2017   \n",
       "\n",
       "                                            sentences  positive_sent_count  \\\n",
       "8   [{'sentence': 'Ayala Corporation’s net profits...                  110   \n",
       "15  [{'sentence': 'The following discussion and an...                   54   \n",
       "3   [{'sentence': 'The following discussion and an...                   62   \n",
       "18  [{'sentence': 'Megaworld, the country’s larges...                   23   \n",
       "10  [{'sentence': 'The following discussion and an...                  201   \n",
       "17  [{'sentence': 'Megaworld, the country’s larges...                   17   \n",
       "11  [{'sentence': 'The following discussion and an...                  155   \n",
       "9   [{'sentence': 'The further easing of quarantin...                  113   \n",
       "16  [{'sentence': 'The following discussion and an...                   40   \n",
       "4   [{'sentence': 'The following discussion and an...                   62   \n",
       "7   [{'sentence': 'Ayala Corporation’s full year e...                  113   \n",
       "19  [{'sentence': 'Megaworld, the country’s larges...                   28   \n",
       "14  [{'sentence': 'The following discussion and an...                  156   \n",
       "2   [{'sentence': 'The following discussion and an...                   79   \n",
       "23  [{'sentence': 'The Company posted consolidated...                   19   \n",
       "20  [{'sentence': 'Megaworld, the country’s larges...                   27   \n",
       "22  [{'sentence': 'The Company posted consolidated...                   13   \n",
       "12  [{'sentence': 'The following discussion and an...                  147   \n",
       "1   [{'sentence': 'The following discussion and an...                   75   \n",
       "6   [{'sentence': 'Ayala Corporation’s net income ...                  124   \n",
       "13  [{'sentence': 'Ayala Corporation recorded a ne...                  119   \n",
       "21  [{'sentence': 'The following discussion and an...                   93   \n",
       "5   [{'sentence': 'The following discussion and an...                  135   \n",
       "0   [{'sentence': 'Megaworld, the country’s larges...                   30   \n",
       "\n",
       "    neutral_sent_count  negative_sent_count  sentiment_score_corpus  \\\n",
       "8                   46                   68                0.187500   \n",
       "15                  26                   73               -0.124183   \n",
       "3                   10                   28                0.340000   \n",
       "18                   7                   21                0.039216   \n",
       "10                  24                   88                0.361022   \n",
       "17                  14                   21               -0.076923   \n",
       "11                  61                   52                0.384328   \n",
       "9                   78                   75                0.142857   \n",
       "16                  16                   26                0.170732   \n",
       "4                   51                   45                0.107595   \n",
       "7                   56                   69                0.184874   \n",
       "19                   5                   22                0.109091   \n",
       "14                  37                   56                0.401606   \n",
       "2                   14                   41                0.283582   \n",
       "23                  22                   25               -0.090909   \n",
       "20                   4                   22                0.094340   \n",
       "22                  32                   24               -0.159420   \n",
       "12                  26                   34                0.545894   \n",
       "1                   14                   49                0.188406   \n",
       "6                   24                   74                0.225225   \n",
       "13                  16                   67                0.257426   \n",
       "21                  23                   34                0.393333   \n",
       "5                   28                  138               -0.009967   \n",
       "0                    4                   19                0.207547   \n",
       "\n",
       "   corpus_interpretation  sentence_count  token_count  \n",
       "8               positive             224         5748  \n",
       "15              negative             153         4303  \n",
       "3               positive             100         2797  \n",
       "18               neutral              51         1642  \n",
       "10              positive             313         8341  \n",
       "17              negative              52         1603  \n",
       "11              positive             268         7140  \n",
       "9               positive             266         6637  \n",
       "16              positive              82         2176  \n",
       "4               positive             158         4044  \n",
       "7               positive             238         5910  \n",
       "19              positive              55         1687  \n",
       "14              positive             249         6622  \n",
       "2               positive             134         3555  \n",
       "23              negative              66         1453  \n",
       "20              positive              53         1868  \n",
       "22              negative              69         1480  \n",
       "12              positive             207         5774  \n",
       "1               positive             138         3351  \n",
       "6               positive             222         6091  \n",
       "13              positive             202         5300  \n",
       "21              positive             150         3605  \n",
       "5                neutral             301         7513  \n",
       "0               positive              53         1800  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas DataFrame from the results list\n",
    "df = pd.DataFrame(results_sent)\n",
    "\n",
    "# Add a column for the total sentence count\n",
    "df['sentence_count'] = df['sentences'].apply(lambda x: len(x))\n",
    "\n",
    "# Add a column for the total token count\n",
    "df['token_count'] = df['sentences'].apply(lambda x: sum(len(sentence['sentence'].split()) for sentence in x))\n",
    "\n",
    "output_dir_path = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_exports'\n",
    "output_file_name = 'sent_lvl.csv'\n",
    "output_file_path = os.path.join(output_dir_path, output_file_name)\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "df = df.sort_values('year', ascending=False)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
