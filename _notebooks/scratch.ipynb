{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re\n",
    "\n",
    "# load the Spacy English language model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# define the path to your file\n",
    "file_path = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_exports/MEG_2017.txt'\n",
    "\n",
    "# open the file and read its contents\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# process the text with the Spacy nlp pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# split the text into sentences using Spacy\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# define a regular expression pattern to match one-word sentences\n",
    "pattern = re.compile(r'^\\w+(\\s+\\w+)*[.?!]$')\n",
    "\n",
    "# filter out sentences that match the pattern\n",
    "filtered_sentences = [sent for sent in sentences if not pattern.match(sent)]\n",
    "\n",
    "# print the filtered sentences\n",
    "for sent in filtered_sentences:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# initialize the FinBERT model and tokenizer\n",
    "model_name = 'ProsusAI/finbert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "for sent in filtered_sentences:\n",
    "    # tokenize the sentence and add special tokens for classification\n",
    "    inputs = tokenizer(sent, return_tensors='pt', padding=True, truncation=True)\n",
    "    # classify the sentiment using the FinBERT model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.detach().numpy()[0]\n",
    "    probs = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "    # print the sentence and its sentiment scores\n",
    "    print(f\"Sentence: {sent}\")\n",
    "    print(f\"Positive sentiment score: {probs[0]:.4f}\")\n",
    "    print(f\"Neutral sentiment score: {probs[1]:.4f}\")\n",
    "    print(f\"Negative sentiment score: {probs[2]:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the Spacy English language model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Initialize the FinBERT model and tokenizer\n",
    "model_name = 'ProsusAI/finbert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the path to the folder containing text files\n",
    "folder_path = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_exports/'\n",
    "\n",
    "# Loop through each file in the folder and process its contents\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        # Open the file and read its contents\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Process the text with the Spacy nlp pipeline\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Split the text into sentences using Spacy\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "        # Define a regular expression pattern to match one-word sentences\n",
    "        pattern = re.compile(r'^\\w+(\\s+\\w+)*[.?!]$')\n",
    "\n",
    "        # Filter out sentences that match the pattern\n",
    "        filtered_sentences = [sent for sent in sentences if not pattern.match(sent)]\n",
    "\n",
    "        # Create an empty DataFrame to store the results\n",
    "        sentiment_df = pd.DataFrame(columns=['sentence', 'positive', 'neutral', 'negative'])\n",
    "\n",
    "        for sent in filtered_sentences:\n",
    "            # Tokenize the sentence and add special tokens for classification\n",
    "            inputs = tokenizer(sent, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # Classify the sentiment using the FinBERT model\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.detach().numpy()[0]\n",
    "            probs = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "\n",
    "            # Save the sentence and its sentiment scores in the DataFrame\n",
    "            new_row = pd.DataFrame({'sentence': [sent], 'positive': [probs[0]], 'neutral': [probs[1]], 'negative': [probs[2]]})\n",
    "            sentiment_df = pd.concat([sentiment_df, new_row], ignore_index=True)\n",
    "\n",
    "        # Save the results to a CSV file with the same name as the input file\n",
    "        output_file_name = file_name.split('.')[0] + '_sentiment.csv'\n",
    "        output_file_path = os.path.join(folder_path, output_file_name)\n",
    "        sentiment_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "        # print a message to indicate that the file has been processed\n",
    "        print(file_name, 'has been processed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load the Spacy English language model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# initialize the FinBERT model and tokenizer\n",
    "model_name = 'ProsusAI/finbert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# set a fixed seed for the random number generator\n",
    "torch.manual_seed(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# define the path to your input folder\n",
    "input_folder = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_exports'\n",
    "\n",
    "# define the path to your output folder\n",
    "output_folder = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_processed'\n",
    "\n",
    "# define a regular expression pattern to match one-word sentences\n",
    "pattern = re.compile(r'^\\w+(\\s+\\w+)*[.?!]$')\n",
    "\n",
    "# iterate over all files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    # define the path to the input file\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "    # open the input file and read its contents\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # process the text with the Spacy nlp pipeline\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # split the text into sentences using Spacy\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    # filter out sentences that match the pattern\n",
    "    filtered_sentences = [sent for sent in sentences if not pattern.match(sent)]\n",
    "\n",
    "    # create an empty DataFrame to store the results\n",
    "    sentiment_df = pd.DataFrame(columns=['sentence', 'positive', 'neutral', 'negative'])\n",
    "\n",
    "    for sent in filtered_sentences:\n",
    "        # tokenize the sentence and add special tokens for classification\n",
    "        inputs = tokenizer(sent, return_tensors='pt', padding=True, truncation=True)\n",
    "        # process using CPU\n",
    "        device = torch.device('cpu')\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        # classify the sentiment using the FinBERT model\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.detach().cpu().numpy()[0]\n",
    "        probs = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "        # save the sentence and its sentiment scores in the DataFrame\n",
    "        new_row = pd.DataFrame({'sentence': [sent], 'positive': [probs[0]], 'neutral': [probs[1]], 'negative': [probs[2]]})\n",
    "        sentiment_df = pd.concat([sentiment_df, new_row], ignore_index=True)\n",
    "\n",
    "    # define the path to the output file\n",
    "    output_file_path = os.path.join(output_folder, file_name.replace('.txt', '.csv'))\n",
    "\n",
    "    # save the results to a CSV file\n",
    "    sentiment_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    # print a message to indicate that the file has been processed\n",
    "    print(f\"{file_name} has been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another test updated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEV_2021.csv has been processed.\n",
      "GLO_2018.csv has been processed.\n",
      "GLO_2019.csv has been processed.\n",
      "AEV_2020.csv has been processed.\n",
      "BPI_2017.csv has been processed.\n",
      ".DS_Store has been processed.\n",
      "BDO_2017.csv has been processed.\n",
      "AC_2017.csv has been processed.\n",
      "AEV_2018.csv has been processed.\n",
      "GLO_2021.csv has been processed.\n",
      "GLO_2020.csv has been processed.\n",
      "AEV_2019.csv has been processed.\n",
      "MEG_2020.csv has been processed.\n",
      "MEG_2021.csv has been processed.\n",
      "ACEN_2021.csv has been processed.\n",
      "ACEN_2020.csv has been processed.\n",
      "ALI_2017.csv has been processed.\n",
      "AGI_2017.csv has been processed.\n",
      "ACEN_2018.csv has been processed.\n",
      "ACEN_2019.csv has been processed.\n",
      "MEG_2019.csv has been processed.\n",
      "AP_2017.csv has been processed.\n",
      "MEG_2018.csv has been processed.\n",
      "ALI_2020.csv has been processed.\n",
      "AGI_2018.csv has been processed.\n",
      "AGI_2019.csv has been processed.\n",
      "ALI_2021.csv has been processed.\n",
      "AP_2019.csv has been processed.\n",
      "AP_2018.csv has been processed.\n",
      "MEG_2017.csv has been processed.\n",
      "AP_2020.csv has been processed.\n",
      "AP_2021.csv has been processed.\n",
      "GTCAP_2017.csv has been processed.\n",
      "ALI_2019.csv has been processed.\n",
      "AGI_2021.csv has been processed.\n",
      "AGI_2020.csv has been processed.\n",
      "ALI_2018.csv has been processed.\n",
      "BDO_2018.csv has been processed.\n",
      "BDO_2019.csv has been processed.\n",
      "AC_2018.csv has been processed.\n",
      "BPI_2020.csv has been processed.\n",
      "AEV_2017.csv has been processed.\n",
      "BPI_2021.csv has been processed.\n",
      "AC_2019.csv has been processed.\n",
      "AC_2021.csv has been processed.\n",
      "BPI_2019.csv has been processed.\n",
      "GLO_2017.csv has been processed.\n",
      "BPI_2018.csv has been processed.\n",
      "AC_2020.csv has been processed.\n",
      "CNVRG_2021.csv has been processed.\n",
      "BDO_2021.csv has been processed.\n",
      "BDO_2020.csv has been processed.\n",
      "CNVRG_2020.csv has been processed.\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load the Spacy English language model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# initialize the FinBERT model and tokenizer\n",
    "model_name = 'ProsusAI/finbert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# set a fixed seed for the random number generator\n",
    "torch.manual_seed(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# define the path to your input folder\n",
    "input_folder = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed'\n",
    "\n",
    "# define the path to your output folder\n",
    "output_folder = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_processed'\n",
    "\n",
    "# add regular expressions to match \"-\" and currency symbols\n",
    "pattern = re.compile(r'^\\w+(\\s+\\w+)*[.?!]?(\\s+\\$)?(\\s+¥)?(\\s+€)?(\\s+£)?(\\s+\\u20B1)?(\\s+-)?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "# iterate over all files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    # define the path to the input file\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "    # read the csv file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # replace NaN values with empty strings\n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    \n",
    "    # get the sentences from the second column of the DataFrame\n",
    "    sentences = df.iloc[:, 1]\n",
    "\n",
    "    # filter out sentences that match the pattern\n",
    "    filtered_sentences = [sent for sent in sentences if not pattern.match(sent)]\n",
    "\n",
    "    # create an empty DataFrame to store the results\n",
    "    sentiment_df = pd.DataFrame(columns=['sentence', 'positive', 'neutral', 'negative'])\n",
    "\n",
    "    for sent in filtered_sentences:\n",
    "        # tokenize the sentence and add special tokens for classification\n",
    "        inputs = tokenizer(sent, return_tensors='pt', padding=True, truncation=True)\n",
    "        # process using CPU\n",
    "        device = torch.device('cpu')\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        # classify the sentiment using the FinBERT model\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.detach().cpu().numpy()[0]\n",
    "        probs = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "        # save the sentence and its sentiment scores in the DataFrame\n",
    "        new_row = pd.DataFrame({'sentence': [sent], 'positive': [probs[0]], 'neutral': [probs[2]], 'negative': [probs[1]]})\n",
    "        sentiment_df = pd.concat([sentiment_df, new_row], ignore_index=True)\n",
    "\n",
    "    # define the path to the output file\n",
    "    output_file_path = os.path.join(output_folder, file_name.replace('.csv', '.csv'))\n",
    "\n",
    "    # save the results to a CSV file\n",
    "    sentiment_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    # print a message to indicate that the file has been processed\n",
    "    print(f\"{file_name} has been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4dfb9f399f4769b1c1d0f64b09aeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434cc3280e864e4fa173c007786990f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/520 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ad86b928c24cc1bac85340001a9fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd693033fdc463bb2968a65f98c49c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd29d7438efa4f8f89c6ad15b26337c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", problem_type=\"multi_label_classification\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "labels = torch.sum(\n",
    "    torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
    ").to(torch.float)\n",
    "loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "\n",
    "# download the NLTK tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# initialize the FinBERT model and tokenizer\n",
    "# model_name = 'ProsusAI/finbert'\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# set a fixed seed for the random number generator\n",
    "torch.manual_seed(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# define the path to your input folder\n",
    "input_folder = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_exports'\n",
    "\n",
    "# define the path to your output folder\n",
    "output_folder = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_processed'\n",
    "\n",
    "# define a regular expression pattern to match one-word sentences\n",
    "pattern = re.compile(r'^\\w+(\\s+\\w+)*[.?!]$')\n",
    "\n",
    "# iterate over all files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    # define the path to the input file\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "    # open the input file and read its contents\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # split the text into sentences using NLTK\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # filter out sentences that match the pattern\n",
    "    filtered_sentences = [sent for sent in sentences if not pattern.match(sent)]\n",
    "\n",
    "    # create an empty DataFrame to store the results\n",
    "    sentiment_df = pd.DataFrame(columns=['sentence', 'positive', 'neutral', 'negative'])\n",
    "\n",
    "    for sent in filtered_sentences:\n",
    "        # tokenize the sentence and add special tokens for classification\n",
    "        inputs = tokenizer(sent, return_tensors='pt', padding=True, truncation=True)\n",
    "        # process using CPU\n",
    "        device = torch.device('cpu')\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        # classify the sentiment using the FinBERT model\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.detach().cpu().numpy()[0]\n",
    "        probs = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "        # save the sentence and its sentiment scores in the DataFrame\n",
    "        new_row = pd.DataFrame({'sentence': [sent], 'positive': [probs[0]], 'neutral': [probs[2]], 'negative': [probs[1]]})\n",
    "        sentiment_df = pd.concat([sentiment_df, new_row], ignore_index=True)\n",
    "\n",
    "    # define the path to the output file\n",
    "    output_file_path = os.path.join(output_folder, file_name.replace('.txt', '.csv'))\n",
    "\n",
    "    # save the results to a CSV file\n",
    "    sentiment_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    # print a message to indicate that the file has been processed\n",
    "    print(f\"{file_name} has been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Positive', 'score': 0.9999780654907227}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"Consolidated EBITDA increased by 8% in 2018, primarily due to the fresh contributions from PEC and Hedcor Bukidnon, which commenced commercial operations in March 2018 and July 2018, respectively, and further augmented by higher contributions from GMCP due to higher availability factor in 2018 as compared to the previous year.\"]\n",
    "results = nlp(sentences)\n",
    "print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Negative', 'score': 0.9966174960136414}, {'label': 'Positive', 'score': 1.0}, {'label': 'Negative', 'score': 0.9999710321426392}, {'label': 'Neutral', 'score': 0.9889442920684814}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]\n",
    "results = nlp(sentences)\n",
    "print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "# set the path to the input folder\n",
    "input_folder = \"/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_exports\"\n",
    "\n",
    "# set the path to the output folder\n",
    "output_folder = \"/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed\"\n",
    "\n",
    "# create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# initialize the NLTK sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# iterate over all files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    # define the path to the input file\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "    # open the input file and read its contents\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # split the text into sentences using NLTK\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "\n",
    "    # create a list of tuples, where each tuple contains the filename and a sentence\n",
    "    rows = [(file_name, sent.strip()) for sent in sentences]\n",
    "\n",
    "    # define the path to the output file\n",
    "    output_file_path = os.path.join(output_folder, file_name.replace('.txt', '.csv'))\n",
    "\n",
    "    # write the list of tuples to a CSV file\n",
    "    with open(output_file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['filename', 'sentence'])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    # print a message to indicate that the file has been processed\n",
    "    print(f\"{file_name} has been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/testing/test_convert_pdf/AC_2017.pdf has been processed and saved as /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/testing/test_convert_output/AC_2017.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# set the path to the input folder\n",
    "input_folder = \"/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/testing/test_convert_pdf\"\n",
    "\n",
    "# set the path to the output folder\n",
    "output_folder = \"/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/testing/test_convert_output\"\n",
    "\n",
    "# create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# iterate over all files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    # check if the file is a PDF file\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        # define the path to the input file\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        # define the path to the output file\n",
    "        output_file_path = os.path.join(output_folder, os.path.splitext(file_name)[0] + \".txt\")\n",
    "        \n",
    "        # extract the text from the PDF file\n",
    "        text = extract_text(input_file_path)\n",
    "        \n",
    "        # find the start and end positions of the target section\n",
    "        start_pos = text.find(\"Management’s Discussion and Analysis\", text.find(\"Management’s Discussion and Analysis\")+1)\n",
    "        end_pos = text.find(\"Financial Statements and Supplementary Schedules\", text.find(\"Financial Statements and Supplementary Schedules\")+1)\n",
    "        \n",
    "        # check if the target section was found\n",
    "        if start_pos == -1 or end_pos == -1:\n",
    "            print(f\"Target section not found in {file_name}.\")\n",
    "            continue\n",
    "        \n",
    "        # extract the target section\n",
    "        target_section = text[start_pos:end_pos]\n",
    "        \n",
    "        # write the extracted text to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(target_section)\n",
    "        \n",
    "        # print a message to indicate that the file has been processed\n",
    "        print(f\"PDF file {input_file_path} has been processed and saved as {output_file_path}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CSV file: AEV_2021.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/AEV_2021.csv has been updated.\n",
      "Processing CSV file: GLO_2018.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/GLO_2018.csv has been updated.\n",
      "Processing CSV file: GLO_2019.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/GLO_2019.csv has been updated.\n",
      "Processing CSV file: AEV_2020.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/AEV_2020.csv has been updated.\n",
      "Processing CSV file: BPI_2017.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/BPI_2017.csv has been updated.\n",
      "Processing CSV file: BDO_2017.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/BDO_2017.csv has been updated.\n",
      "Processing CSV file: AC_2017.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/AC_2017.csv has been updated.\n",
      "Processing CSV file: AEV_2018.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/AEV_2018.csv has been updated.\n",
      "Processing CSV file: GLO_2021.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/GLO_2021.csv has been updated.\n",
      "Processing CSV file: GLO_2020.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/GLO_2020.csv has been updated.\n",
      "Processing CSV file: AEV_2019.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/AEV_2019.csv has been updated.\n",
      "Processing CSV file: MEG_2020.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/MEG_2020.csv has been updated.\n",
      "Processing CSV file: MEG_2021.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/MEG_2021.csv has been updated.\n",
      "Processing CSV file: ACEN_2021.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/ACEN_2021.csv has been updated.\n",
      "Processing CSV file: ACEN_2020.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/ACEN_2020.csv has been updated.\n",
      "Processing CSV file: ALI_2017.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/ALI_2017.csv has been updated.\n",
      "Processing CSV file: AGI_2017.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/AGI_2017.csv has been updated.\n",
      "Processing CSV file: ACEN_2018.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/ACEN_2018.csv has been updated.\n",
      "Processing CSV file: ACEN_2019.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/ACEN_2019.csv has been updated.\n",
      "Processing CSV file: MEG_2019.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/MEG_2019.csv has been updated.\n",
      "Processing CSV file: AP_2017.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/AP_2017.csv has been updated.\n",
      "Processing CSV file: MEG_2018.csv\n",
      "CSV file /Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed/MEG_2018.csv has been updated.\n",
      "Processing CSV file: ALI_2020.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'year', 'ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictWriter(fh, fieldnames\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m     writer\u001b[39m.\u001b[39mwriteheader()\n\u001b[0;32m---> 26\u001b[0m     writer\u001b[39m.\u001b[39;49mwriterows(rows)\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCSV file \u001b[39m\u001b[39m{\u001b[39;00minput_file_path\u001b[39m}\u001b[39;00m\u001b[39m has been updated.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/msfin299/lib/python3.9/csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwriterows\u001b[39m(\u001b[39mself\u001b[39m, rowdicts):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mwriterows(\u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dict_to_list, rowdicts))\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/msfin299/lib/python3.9/csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    147\u001b[0m     wrong_fields \u001b[39m=\u001b[39m rowdict\u001b[39m.\u001b[39mkeys() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m wrong_fields:\n\u001b[0;32m--> 149\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdict contains fields not in fieldnames: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m                          \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mrepr\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m wrong_fields]))\n\u001b[1;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m (rowdict\u001b[39m.\u001b[39mget(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestval) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames)\n",
      "\u001b[0;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'year', 'ticker'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "input_folder = \"/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/_data/17a_scrubbed\"\n",
    "\n",
    "# iterate over all CSV files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        print(f\"Processing CSV file: {file_name}\")\n",
    "        # define the path to the input file\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        # read the data from the CSV file\n",
    "        with open(input_file_path, newline='', encoding='utf-8') as fh:\n",
    "            reader = csv.DictReader(fh)\n",
    "            rows = list(reader)\n",
    "\n",
    "        # update the sentences by removing newlines\n",
    "        for row in rows:\n",
    "            row['sentence'] = row['sentence'].replace('\\n', ' ')\n",
    "\n",
    "        # write the updated data back to the CSV file\n",
    "        with open(input_file_path, 'w', newline='', encoding='utf-8') as fh:\n",
    "            writer = csv.DictWriter(fh, fieldnames=['filename', 'sentence'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "\n",
    "        print(f\"CSV file {input_file_path} has been updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msfin299",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
