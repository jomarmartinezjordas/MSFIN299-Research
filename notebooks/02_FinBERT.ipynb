{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis using FinBERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing classes, directories, and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy English language model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentiment classes and the corresponding sentiment scores\n",
    "sentiment_classes = {0: 'Positive', 1: 'Neutral', 2: 'Negative'}\n",
    "sentiment_scores = {0: 1, 1: 0, 2: -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing the CSV files\n",
    "dir_path = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/data/17a_exports'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FinBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the input into the FinBERT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results_doc = []\n",
    "\n",
    "# Iterate through all the text files in the directory\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        \n",
    "        # Read the contents of the text file\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read().replace('\\n', '')\n",
    "        \n",
    "        # Tokenize the text using the BERT tokenizer\n",
    "        tokens = tokenizer.encode_plus(text, max_length=512, truncation=True, padding='max_length',\n",
    "                                       add_special_tokens=True, return_tensors='pt')\n",
    "        \n",
    "        # Get the model's output for the tokenized text\n",
    "        output = model(**tokens)\n",
    "        \n",
    "        # Apply softmax to the logits output tensor of our model (in index 0) across dimension -1\n",
    "        probs = F.softmax(output[0], dim=-1)\n",
    "        \n",
    "        # Get the index of the predicted sentiment class\n",
    "        pred_class_idx = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        # Map the predicted sentiment class to a sentiment score and interpretation\n",
    "        sentiment_score = sentiment_scores[pred_class_idx.item()]\n",
    "        interpretation = sentiment_classes[pred_class_idx.item()]\n",
    "        \n",
    "        # Add the sentiment score and interpretation to the list of results\n",
    "        results_doc.append({'file_name': filename, 'sentiment_score': sentiment_score, 'interpretation': interpretation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      file_name  sentiment_score interpretation\n",
      "0  MEG_2017.txt                1       Positive\n",
      "1  MEG_2020.txt                0        Neutral\n",
      "2  MEG_2021.txt                0        Neutral\n",
      "3  MEG_2019.txt                1       Positive\n",
      "4  MEG_2018.txt                1       Positive\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with the sentiment score and interpretation for each file\n",
    "df_results_doc = pd.DataFrame(results_doc)\n",
    "\n",
    "# Print the results dataframe\n",
    "print(df_results_doc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentiment scores and classes\n",
    "sentiment_scores_sent = [1, 0, -1]\n",
    "sentiment_classes_sent = ['positive', 'neutral', 'negative']\n",
    "\n",
    "# Initialize the results list and counters\n",
    "results_sent = []\n",
    "pos_count = 0\n",
    "neu_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "# Loop over each text file in the directory\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "        # Read the contents of the text file\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Use Spacy to split the text into sentences\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # Initialize the sentence-level results list and counters\n",
    "        sentence_results = []\n",
    "        pos_sent_count = 0\n",
    "        neu_sent_count = 0\n",
    "        neg_sent_count = 0\n",
    "\n",
    "        # Loop over each sentence in the text file\n",
    "        for sentence in sentences:\n",
    "            # Tokenize the sentence using the FinBERT tokenizer\n",
    "            tokens = tokenizer.encode_plus(sentence, max_length=512, truncation=True, padding='max_length',\n",
    "                                           add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "            # Get the model's output for the tokenized sentence\n",
    "            output = model(**tokens)\n",
    "\n",
    "            # Apply softmax to the logits output tensor of our model (in index 0) across dimension -1\n",
    "            probs = F.softmax(output[0], dim=-1)\n",
    "\n",
    "            # Get the index of the predicted sentiment class\n",
    "            pred_class_idx = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Map the predicted sentiment class to a sentiment score and interpretation\n",
    "            sentiment_score = sentiment_scores_sent[pred_class_idx.item()]\n",
    "            interpretation = sentiment_classes_sent[pred_class_idx.item()]\n",
    "\n",
    "            # Add the sentiment score and interpretation to the sentence-level results list\n",
    "            sentence_results.append({'sentence': sentence.strip(), 'sentiment_score': sentiment_score, 'interpretation': interpretation})\n",
    "\n",
    "            # Update the sentence-level sentiment counters\n",
    "            if interpretation == 'positive':\n",
    "                pos_sent_count += 1\n",
    "            elif interpretation == 'neutral':\n",
    "                neu_sent_count += 1\n",
    "            elif interpretation == 'negative':\n",
    "                neg_sent_count += 1\n",
    "\n",
    "        # Calculate the overall sentiment score for the corpus\n",
    "        sentiment_score_corpus = (pos_sent_count - neg_sent_count) / len(sentences)\n",
    "\n",
    "        # Add the sentence-level and file-level results to the results list\n",
    "        results_sent.append({'file_name': filename, 'sentences': sentence_results, \n",
    "                              'positive_sent_count': pos_sent_count,\n",
    "                              'neutral_sent_count': neu_sent_count,\n",
    "                              'negative_sent_count': neg_sent_count,\n",
    "                              'total_sent_count': len(sentences),\n",
    "                              'sentiment_score_corpus': sentiment_score_corpus})\n",
    "        \n",
    "        # Update the file-level sentiment counters\n",
    "        pos_count += pos_sent_count\n",
    "        neu_count += neu_sent_count\n",
    "        neg_count += neg_sent_count\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>sentences</th>\n",
       "      <th>positive_sent_count</th>\n",
       "      <th>neutral_sent_count</th>\n",
       "      <th>negative_sent_count</th>\n",
       "      <th>total_sent_count</th>\n",
       "      <th>sentiment_score_corpus</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEG_2017.txt</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>53</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>53</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEG_2020.txt</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>52</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>52</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEG_2021.txt</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>52</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>52</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEG_2019.txt</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>55</td>\n",
       "      <td>1687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEG_2018.txt</td>\n",
       "      <td>[{'sentence': 'Megaworld, the country’s larges...</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>53</td>\n",
       "      <td>1868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_name                                          sentences  \\\n",
       "0  MEG_2017.txt  [{'sentence': 'Megaworld, the country’s larges...   \n",
       "1  MEG_2020.txt  [{'sentence': 'Megaworld, the country’s larges...   \n",
       "2  MEG_2021.txt  [{'sentence': 'Megaworld, the country’s larges...   \n",
       "3  MEG_2019.txt  [{'sentence': 'Megaworld, the country’s larges...   \n",
       "4  MEG_2018.txt  [{'sentence': 'Megaworld, the country’s larges...   \n",
       "\n",
       "   positive_sent_count  neutral_sent_count  negative_sent_count  \\\n",
       "0                   30                   4                   19   \n",
       "1                   17                  14                   21   \n",
       "2                   17                  14                   21   \n",
       "3                   28                   5                   22   \n",
       "4                   27                   4                   22   \n",
       "\n",
       "   total_sent_count  sentiment_score_corpus  sentence_count  token_count  \n",
       "0                53                0.207547              53         1800  \n",
       "1                52               -0.076923              52         1603  \n",
       "2                52               -0.076923              52         1603  \n",
       "3                55                0.109091              55         1687  \n",
       "4                53                0.094340              53         1868  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas DataFrame from the results list\n",
    "df = pd.DataFrame(results_sent)\n",
    "\n",
    "# Add a column for the total sentence count\n",
    "df['sentence_count'] = df['sentences'].apply(lambda x: len(x))\n",
    "\n",
    "# Add a column for the total token count\n",
    "df['token_count'] = df['sentences'].apply(lambda x: sum(len(sentence['sentence'].split()) for sentence in x))\n",
    "\n",
    "output_dir_path = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/data/17a_exports'\n",
    "output_file_name = 'sent_lvl.csv'\n",
    "output_file_path = os.path.join(output_dir_path, output_file_name)\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Users/jomarjordas/Documents/MSFIN299/MSFIN299-Research/data/17a_scrubbed'\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"The data type of {filename} is {type(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
